{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9PtLOtOa1FO2"
      },
      "source": [
        "## Fine tuning poem model on Joy poems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "JaTfjcvNtfEQ",
        "outputId": "9162bca9-2b0e-4fb0-ad47-c79e1f90b7fd",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
            "[INFO|configuration_utils.py:668] 2023-03-15 22:20:51,416 >> loading configuration file config.json from cache at C:\\Users\\wangz/.cache\\huggingface\\hub\\models--ismaelfaro--gpt2-poems.en\\snapshots\\0cf19ae5b1500bd3cd172571628596aa085655b3\\config.json\n",
            "[INFO|configuration_utils.py:720] 2023-03-15 22:20:51,417 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"ismaelfaro/gpt2-poems.en\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.27.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-03-15 22:20:51,795 >> loading file vocab.json from cache at C:\\Users\\wangz/.cache\\huggingface\\hub\\models--ismaelfaro--gpt2-poems.en\\snapshots\\0cf19ae5b1500bd3cd172571628596aa085655b3\\vocab.json\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-03-15 22:20:51,795 >> loading file merges.txt from cache at C:\\Users\\wangz/.cache\\huggingface\\hub\\models--ismaelfaro--gpt2-poems.en\\snapshots\\0cf19ae5b1500bd3cd172571628596aa085655b3\\merges.txt\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-03-15 22:20:51,795 >> loading file tokenizer.json from cache at C:\\Users\\wangz/.cache\\huggingface\\hub\\models--ismaelfaro--gpt2-poems.en\\snapshots\\0cf19ae5b1500bd3cd172571628596aa085655b3\\tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-03-15 22:20:51,795 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-03-15 22:20:51,795 >> loading file special_tokens_map.json from cache at C:\\Users\\wangz/.cache\\huggingface\\hub\\models--ismaelfaro--gpt2-poems.en\\snapshots\\0cf19ae5b1500bd3cd172571628596aa085655b3\\special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-03-15 22:20:51,795 >> loading file tokenizer_config.json from cache at C:\\Users\\wangz/.cache\\huggingface\\hub\\models--ismaelfaro--gpt2-poems.en\\snapshots\\0cf19ae5b1500bd3cd172571628596aa085655b3\\tokenizer_config.json\n",
            "[INFO|modeling_utils.py:2403] 2023-03-15 22:20:51,843 >> loading weights file pytorch_model.bin from cache at C:\\Users\\wangz/.cache\\huggingface\\hub\\models--ismaelfaro--gpt2-poems.en\\snapshots\\0cf19ae5b1500bd3cd172571628596aa085655b3\\pytorch_model.bin\n",
            "[INFO|configuration_utils.py:575] 2023-03-15 22:20:52,053 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.27.0\"\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3032] 2023-03-15 22:20:53,053 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:3041] 2023-03-15 22:20:53,053 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ismaelfaro/gpt2-poems.en.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "[INFO|modeling_utils.py:2691] 2023-03-15 22:20:53,519 >> Generation config file not found, using a generation config created from the model config.\n",
            "C:\\Users\\wangz\\.conda\\envs\\MRI\\lib\\site-packages\\transformers\\optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1740] 2023-03-15 22:20:54,723 >> ***** Running training *****\n",
            "[INFO|trainer.py:1741] 2023-03-15 22:20:54,723 >>   Num examples = 94\n",
            "[INFO|trainer.py:1742] 2023-03-15 22:20:54,723 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1743] 2023-03-15 22:20:54,723 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:1744] 2023-03-15 22:20:54,723 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "[INFO|trainer.py:1745] 2023-03-15 22:20:54,723 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1746] 2023-03-15 22:20:54,723 >>   Total optimization steps = 72\n",
            "[INFO|trainer.py:1748] 2023-03-15 22:20:54,724 >>   Number of trainable parameters = 124439808\n",
            "\n",
            "  0%|          | 0/72 [00:00<?, ?it/s]Traceback (most recent call last):\n",
            "  File \"fine_tune.py\", line 626, in <module>\n",
            "    main()\n",
            "  File \"fine_tune.py\", line 574, in main\n",
            "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
            "  File \"C:\\Users\\wangz\\.conda\\envs\\MRI\\lib\\site-packages\\transformers\\trainer.py\", line 1637, in train\n",
            "    ignore_keys_for_eval=ignore_keys_for_eval,\n",
            "  File \"C:\\Users\\wangz\\.conda\\envs\\MRI\\lib\\site-packages\\transformers\\trainer.py\", line 1902, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"C:\\Users\\wangz\\.conda\\envs\\MRI\\lib\\site-packages\\transformers\\trainer.py\", line 2645, in training_step\n",
            "    loss = self.compute_loss(model, inputs)\n",
            "  File \"C:\\Users\\wangz\\.conda\\envs\\MRI\\lib\\site-packages\\transformers\\trainer.py\", line 2677, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"C:\\Users\\wangz\\.conda\\envs\\MRI\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"C:\\Users\\wangz\\.conda\\envs\\MRI\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 1088, in forward\n",
            "    return_dict=return_dict,\n",
            "  File \"C:\\Users\\wangz\\.conda\\envs\\MRI\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"C:\\Users\\wangz\\.conda\\envs\\MRI\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 907, in forward\n",
            "    output_attentions=output_attentions,\n",
            "  File \"C:\\Users\\wangz\\.conda\\envs\\MRI\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"C:\\Users\\wangz\\.conda\\envs\\MRI\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 395, in forward\n",
            "    output_attentions=output_attentions,\n",
            "  File \"C:\\Users\\wangz\\.conda\\envs\\MRI\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"C:\\Users\\wangz\\.conda\\envs\\MRI\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 330, in forward\n",
            "    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
            "  File \"C:\\Users\\wangz\\.conda\\envs\\MRI\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 186, in _attn\n",
            "    [], value.size(-1) ** 0.5, dtype=attn_weights.dtype, device=attn_weights.device\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 8.00 GiB total capacity; 7.09 GiB already allocated; 0 bytes free; 7.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "\n",
            "  0%|          | 0/72 [00:04<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "03/15/2023 22:20:45 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "03/15/2023 22:20:45 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/tmp/poem_baseline_local\\runs\\Mar15_22-20-45_DESKTOP-0QF7VTC,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=/tmp/poem_baseline_local,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=4,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/tmp/poem_baseline_local,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "03/15/2023 22:20:47 - INFO - datasets.builder - Using custom data configuration Ozziey--poems_dataset-d150b0a822ff0b4c\n",
            "03/15/2023 22:20:47 - INFO - datasets.info - Loading Dataset Infos from C:\\Users\\wangz\\.conda\\envs\\MRI\\lib\\site-packages\\datasets\\packaged_modules\\csv\n",
            "03/15/2023 22:20:47 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "03/15/2023 22:20:47 - INFO - datasets.info - Loading Dataset info from C:\\Users\\wangz\\.cache\\huggingface\\datasets\\Ozziey___csv\\Ozziey--poems_dataset-d150b0a822ff0b4c\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n",
            "03/15/2023 22:20:47 - WARNING - datasets.builder - Found cached dataset csv (C:/Users/wangz/.cache/huggingface/datasets/Ozziey___csv/Ozziey--poems_dataset-d150b0a822ff0b4c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
            "03/15/2023 22:20:47 - INFO - datasets.info - Loading Dataset info from C:/Users/wangz/.cache/huggingface/datasets/Ozziey___csv/Ozziey--poems_dataset-d150b0a822ff0b4c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n",
            "03/15/2023 22:20:49 - INFO - datasets.builder - Using custom data configuration Ozziey--poems_dataset-d150b0a822ff0b4c\n",
            "03/15/2023 22:20:49 - INFO - datasets.info - Loading Dataset Infos from C:\\Users\\wangz\\.conda\\envs\\MRI\\lib\\site-packages\\datasets\\packaged_modules\\csv\n",
            "03/15/2023 22:20:49 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "03/15/2023 22:20:49 - INFO - datasets.info - Loading Dataset info from C:\\Users\\wangz\\.cache\\huggingface\\datasets\\Ozziey___csv\\Ozziey--poems_dataset-d150b0a822ff0b4c\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n",
            "03/15/2023 22:20:49 - WARNING - datasets.builder - Found cached dataset csv (C:/Users/wangz/.cache/huggingface/datasets/Ozziey___csv/Ozziey--poems_dataset-d150b0a822ff0b4c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
            "03/15/2023 22:20:49 - INFO - datasets.info - Loading Dataset info from C:/Users/wangz/.cache/huggingface/datasets/Ozziey___csv/Ozziey--poems_dataset-d150b0a822ff0b4c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n",
            "03/15/2023 22:20:51 - INFO - datasets.builder - Using custom data configuration Ozziey--poems_dataset-d150b0a822ff0b4c\n",
            "03/15/2023 22:20:51 - INFO - datasets.info - Loading Dataset Infos from C:\\Users\\wangz\\.conda\\envs\\MRI\\lib\\site-packages\\datasets\\packaged_modules\\csv\n",
            "03/15/2023 22:20:51 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "03/15/2023 22:20:51 - INFO - datasets.info - Loading Dataset info from C:\\Users\\wangz\\.cache\\huggingface\\datasets\\Ozziey___csv\\Ozziey--poems_dataset-d150b0a822ff0b4c\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n",
            "03/15/2023 22:20:51 - WARNING - datasets.builder - Found cached dataset csv (C:/Users/wangz/.cache/huggingface/datasets/Ozziey___csv/Ozziey--poems_dataset-d150b0a822ff0b4c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
            "03/15/2023 22:20:51 - INFO - datasets.info - Loading Dataset info from C:/Users/wangz/.cache/huggingface/datasets/Ozziey___csv/Ozziey--poems_dataset-d150b0a822ff0b4c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n",
            "03/15/2023 22:20:53 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at C:\\Users\\wangz\\.cache\\huggingface\\datasets\\Ozziey___csv\\Ozziey--poems_dataset-d150b0a822ff0b4c\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-f00b659b167788cf.arrow\n",
            "03/15/2023 22:20:53 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at C:\\Users\\wangz\\.cache\\huggingface\\datasets\\Ozziey___csv\\Ozziey--poems_dataset-d150b0a822ff0b4c\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-1b72592bf90923c3.arrow\n",
            "03/15/2023 22:20:53 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at C:\\Users\\wangz\\.cache\\huggingface\\datasets\\Ozziey___csv\\Ozziey--poems_dataset-d150b0a822ff0b4c\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-d7e1093be740fb0f.arrow\n",
            "03/15/2023 22:20:53 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at C:\\Users\\wangz\\.cache\\huggingface\\datasets\\Ozziey___csv\\Ozziey--poems_dataset-d150b0a822ff0b4c\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-12c211d38311fe9b.arrow\n"
          ]
        }
      ],
      "source": [
        "!python fine_tune.py \\\n",
        "--per_device_train_batch_size 4 \\\n",
        "--per_device_eval_batch_size 4 \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--output_dir /tmp/poem_baseline_local \\\n",
        "--overwrite_output_dir"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "n_WakQt07-sL",
        "outputId": "dcaace76-d490-4517-867b-3f42dd6b8004",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== GENERATED SEQUENCE 1 ===\n",
            "fuck for the wrong that they wrought,  \\nBut they did not repent it, not repent it;  \\nThen I said, It must be sin, That I must die:  \\nAnd my friends loved me at the last;\n",
            "=== GENERATED SEQUENCE 2 ===\n",
            "fuck's, and his faith; the fate that hath torn his marriage, yet it lives and dies; the world, when, for men made so much of strife and pain, would still love him in his state.This, by that, she had\n",
            "=== GENERATED SEQUENCE 3 ===\n",
            "fuck.  \\nThe world has many ways of making you look the fool  \\nBut the ones that work the best for your profit.  \\nA good relationship is built not on words alone  \\nBut if you can build a\n",
            "=== GENERATED SEQUENCE 4 ===\n",
            "fuck.and is, will, can, .but, of, not.to.be.the cut.to, of the cut.is, of, not.to.be.the cut.will, to.be.the\n",
            "=== GENERATED SEQUENCE 5 ===\n",
            "fuck is the world.And it is not your me.  \\nNor the world at all  \\nIt is only yours and it is not mine;  \\nNot in the world's round world.Dear, you're never told.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "03/15/2023 22:28:11 - WARNING - __main__ - device: cuda, n_gpu: 1, 16-bits training: False\n",
            "03/15/2023 22:28:13 - INFO - __main__ - Namespace(device=device(type='cuda'), fp16=False, k=50, length=50, model_name_or_path='/tmp/poem_baseline_colab', model_type='gpt2', n_gpu=1, no_cuda=False, num_return_sequences=5, p=0.9, padding_text='', prefix='', prompt='fuck', repetition_penalty=1.0, seed=42, stop_token=None, temperature=1.0, xlm_language='')\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "K=50\n",
        "\n",
        "!python text_generation.py \\\n",
        "--model_type gpt2 \\\n",
        "--model_name_or_path /tmp/poem_baseline_colab \\\n",
        "--length 50 \\\n",
        "--prompt \" \" \\\n",
        "--k $K \\\n",
        "--num_return_sequences 5"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOL4rf+yBkuBNzWQG3oZD5Q",
      "collapsed_sections": [],
      "include_colab_link": true,
      "mount_file_id": "1SsYKniB48_XJTuSX26nRFWYa9Y51BMay",
      "name": "text-generation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
