{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9PtLOtOa1FO2"
      },
      "source": [
        "## Fine tuning GPT-2 language model on our dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VB47o4Jn0N9T"
      },
      "source": [
        "Add to transformers/example/language-modeling/run-language-modeling.py file the following snippet : <br>\n",
        "\n",
        "```\n",
        "special_tokens_dict = {'bos_token': '<BOS>', 'eos_token': '<EOS>', 'pad_token': '<PAD>'}\n",
        "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "JaTfjcvNtfEQ",
        "outputId": "9162bca9-2b0e-4fb0-ad47-c79e1f90b7fd",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "03/15/2023 17:33:42 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "03/15/2023 17:33:42 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/tmp/poem_baseline\\runs\\Mar15_17-33-41_DESKTOP-0QF7VTC,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=/tmp/poem_baseline,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=[],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/tmp/poem_baseline,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "03/15/2023 17:33:44 - INFO - datasets.builder - Using custom data configuration Ozziey--poems_dataset-d150b0a822ff0b4c\n",
            "03/15/2023 17:33:44 - INFO - datasets.info - Loading Dataset Infos from C:\\Users\\wangz\\.conda\\envs\\NLP\\lib\\site-packages\\datasets\\packaged_modules\\csv\n",
            "03/15/2023 17:33:44 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "03/15/2023 17:33:44 - INFO - datasets.info - Loading Dataset info from C:\\Users\\wangz\\.cache\\huggingface\\datasets\\Ozziey___csv\\Ozziey--poems_dataset-d150b0a822ff0b4c\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n",
            "03/15/2023 17:33:44 - WARNING - datasets.builder - Found cached dataset csv (C:/Users/wangz/.cache/huggingface/datasets/Ozziey___csv/Ozziey--poems_dataset-d150b0a822ff0b4c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
            "03/15/2023 17:33:44 - INFO - datasets.info - Loading Dataset info from C:/Users/wangz/.cache/huggingface/datasets/Ozziey___csv/Ozziey--poems_dataset-d150b0a822ff0b4c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n",
            "03/15/2023 17:33:46 - INFO - datasets.builder - Using custom data configuration Ozziey--poems_dataset-d150b0a822ff0b4c\n",
            "03/15/2023 17:33:46 - INFO - datasets.info - Loading Dataset Infos from C:\\Users\\wangz\\.conda\\envs\\NLP\\lib\\site-packages\\datasets\\packaged_modules\\csv\n",
            "03/15/2023 17:33:46 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "03/15/2023 17:33:46 - INFO - datasets.info - Loading Dataset info from C:\\Users\\wangz\\.cache\\huggingface\\datasets\\Ozziey___csv\\Ozziey--poems_dataset-d150b0a822ff0b4c\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n",
            "03/15/2023 17:33:46 - WARNING - datasets.builder - Found cached dataset csv (C:/Users/wangz/.cache/huggingface/datasets/Ozziey___csv/Ozziey--poems_dataset-d150b0a822ff0b4c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
            "03/15/2023 17:33:46 - INFO - datasets.info - Loading Dataset info from C:/Users/wangz/.cache/huggingface/datasets/Ozziey___csv/Ozziey--poems_dataset-d150b0a822ff0b4c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n",
            "03/15/2023 17:33:49 - INFO - datasets.builder - Using custom data configuration Ozziey--poems_dataset-d150b0a822ff0b4c\n",
            "03/15/2023 17:33:49 - INFO - datasets.info - Loading Dataset Infos from C:\\Users\\wangz\\.conda\\envs\\NLP\\lib\\site-packages\\datasets\\packaged_modules\\csv\n",
            "03/15/2023 17:33:49 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "03/15/2023 17:33:49 - INFO - datasets.info - Loading Dataset info from C:\\Users\\wangz\\.cache\\huggingface\\datasets\\Ozziey___csv\\Ozziey--poems_dataset-d150b0a822ff0b4c\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n",
            "03/15/2023 17:33:49 - WARNING - datasets.builder - Found cached dataset csv (C:/Users/wangz/.cache/huggingface/datasets/Ozziey___csv/Ozziey--poems_dataset-d150b0a822ff0b4c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
            "03/15/2023 17:33:49 - INFO - datasets.info - Loading Dataset info from C:/Users/wangz/.cache/huggingface/datasets/Ozziey___csv/Ozziey--poems_dataset-d150b0a822ff0b4c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n",
            "03/15/2023 17:34:11 - INFO - datasets.arrow_dataset - Caching processed dataset at C:\\Users\\wangz\\.cache\\huggingface\\datasets\\Ozziey___csv\\Ozziey--poems_dataset-d150b0a822ff0b4c\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-f00b659b167788cf.arrow\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 956.95it/s]\n",
            "[INFO|configuration_utils.py:668] 2023-03-15 17:33:50,305 >> loading configuration file config.json from cache at C:\\Users\\wangz/.cache\\huggingface\\hub\\models--ismaelfaro--gpt2-poems.en\\snapshots\\0cf19ae5b1500bd3cd172571628596aa085655b3\\config.json\n",
            "[INFO|configuration_utils.py:720] 2023-03-15 17:33:50,306 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"ismaelfaro/gpt2-poems.en\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.27.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-03-15 17:33:50,765 >> loading file vocab.json from cache at C:\\Users\\wangz/.cache\\huggingface\\hub\\models--ismaelfaro--gpt2-poems.en\\snapshots\\0cf19ae5b1500bd3cd172571628596aa085655b3\\vocab.json\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-03-15 17:33:50,765 >> loading file merges.txt from cache at C:\\Users\\wangz/.cache\\huggingface\\hub\\models--ismaelfaro--gpt2-poems.en\\snapshots\\0cf19ae5b1500bd3cd172571628596aa085655b3\\merges.txt\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-03-15 17:33:50,765 >> loading file tokenizer.json from cache at C:\\Users\\wangz/.cache\\huggingface\\hub\\models--ismaelfaro--gpt2-poems.en\\snapshots\\0cf19ae5b1500bd3cd172571628596aa085655b3\\tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-03-15 17:33:50,765 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-03-15 17:33:50,765 >> loading file special_tokens_map.json from cache at C:\\Users\\wangz/.cache\\huggingface\\hub\\models--ismaelfaro--gpt2-poems.en\\snapshots\\0cf19ae5b1500bd3cd172571628596aa085655b3\\special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-03-15 17:33:50,765 >> loading file tokenizer_config.json from cache at C:\\Users\\wangz/.cache\\huggingface\\hub\\models--ismaelfaro--gpt2-poems.en\\snapshots\\0cf19ae5b1500bd3cd172571628596aa085655b3\\tokenizer_config.json\n",
            "[INFO|modeling_utils.py:2403] 2023-03-15 17:34:08,956 >> loading weights file pytorch_model.bin from cache at C:\\Users\\wangz/.cache\\huggingface\\hub\\models--ismaelfaro--gpt2-poems.en\\snapshots\\0cf19ae5b1500bd3cd172571628596aa085655b3\\pytorch_model.bin\n",
            "[INFO|configuration_utils.py:575] 2023-03-15 17:34:09,269 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.27.0\"\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3032] 2023-03-15 17:34:10,643 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:3041] 2023-03-15 17:34:10,644 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ismaelfaro/gpt2-poems.en.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "[INFO|modeling_utils.py:2691] 2023-03-15 17:34:11,022 >> Generation config file not found, using a generation config created from the model config.\n",
            "\n",
            "Running tokenizer on dataset:   0%|          | 0/428 [00:00<?, ? examples/s]\n",
            "Running tokenizer on dataset: 100%|██████████| 428/428 [00:00<00:00, 652.51 examples/s]\n",
            "                                                                                       \n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\wangz\\.conda\\envs\\NLP\\lib\\shutil.py\", line 566, in move\n",
            "    os.rename(src, real_dst)\n",
            "FileExistsError: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\wangz\\\\.cache\\\\huggingface\\\\datasets\\\\Ozziey___csv\\\\Ozziey--poems_dataset-d150b0a822ff0b4c\\\\0.0.0\\\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\\\tmp61dtnjse' -> 'C:\\\\Users\\\\wangz\\\\.cache\\\\huggingface\\\\datasets\\\\Ozziey___csv\\\\Ozziey--poems_dataset-d150b0a822ff0b4c\\\\0.0.0\\\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\\\cache-f00b659b167788cf.arrow'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"fine_tune.py\", line 625, in <module>\n",
            "    main()\n",
            "  File \"fine_tune.py\", line 453, in main\n",
            "    desc=\"Running tokenizer on dataset\",\n",
            "  File \"C:\\Users\\wangz\\.conda\\envs\\NLP\\lib\\site-packages\\datasets\\dataset_dict.py\", line 872, in map\n",
            "    for k, dataset in self.items()\n",
            "  File \"C:\\Users\\wangz\\.conda\\envs\\NLP\\lib\\site-packages\\datasets\\dataset_dict.py\", line 872, in <dictcomp>\n",
            "    for k, dataset in self.items()\n",
            "  File \"C:\\Users\\wangz\\.conda\\envs\\NLP\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 563, in wrapper\n",
            "    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n",
            "  File \"C:\\Users\\wangz\\.conda\\envs\\NLP\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 528, in wrapper\n",
            "    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n",
            "  File \"C:\\Users\\wangz\\.conda\\envs\\NLP\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 2953, in map\n",
            "    for rank, done, content in Dataset._map_single(**dataset_kwargs):\n",
            "  File \"C:\\Users\\wangz\\.conda\\envs\\NLP\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 3368, in _map_single\n",
            "    shutil.move(tmp_file.name, cache_file_name)\n",
            "  File \"C:\\Users\\wangz\\.conda\\envs\\NLP\\lib\\shutil.py\", line 580, in move\n",
            "    copy_function(src, real_dst)\n",
            "  File \"C:\\Users\\wangz\\.conda\\envs\\NLP\\lib\\shutil.py\", line 266, in copy2\n",
            "    copyfile(src, dst, follow_symlinks=follow_symlinks)\n",
            "  File \"C:\\Users\\wangz\\.conda\\envs\\NLP\\lib\\shutil.py\", line 121, in copyfile\n",
            "    with open(dst, 'wb') as fdst:\n",
            "OSError: [Errno 22] Invalid argument: 'C:\\\\Users\\\\wangz\\\\.cache\\\\huggingface\\\\datasets\\\\Ozziey___csv\\\\Ozziey--poems_dataset-d150b0a822ff0b4c\\\\0.0.0\\\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\\\cache-f00b659b167788cf.arrow'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "03/15/2023 17:33:26 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "03/15/2023 17:33:26 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/tmp/poem_baseline\\runs\\Mar15_17-33-26_DESKTOP-0QF7VTC,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=/tmp/poem_baseline,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=[],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/tmp/poem_baseline,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "03/15/2023 17:33:28 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/Ozziey/poems_dataset/resolve/main/README.md not found in cache or force_download set to True, downloading to C:\\Users\\wangz\\.cache\\huggingface\\datasets\\downloads\\tmpamr4j0oz\n",
            "03/15/2023 17:33:29 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/Ozziey/poems_dataset/resolve/main/README.md in cache at C:\\Users\\wangz\\.cache\\huggingface\\datasets\\downloads\\c870e7b550316e3b4002d8143a842cc1fad7c60675c09a80b94c3d38220389ba.1b82478556af4966c9265e13c406905b75652f6431a3854ebf3a8e38bf1b49d7\n",
            "03/15/2023 17:33:29 - INFO - datasets.utils.file_utils - creating metadata file for C:\\Users\\wangz\\.cache\\huggingface\\datasets\\downloads\\c870e7b550316e3b4002d8143a842cc1fad7c60675c09a80b94c3d38220389ba.1b82478556af4966c9265e13c406905b75652f6431a3854ebf3a8e38bf1b49d7\n",
            "03/15/2023 17:33:29 - INFO - datasets.builder - Using custom data configuration Ozziey--poems_dataset-d150b0a822ff0b4c\n",
            "03/15/2023 17:33:29 - INFO - datasets.info - Loading Dataset Infos from C:\\Users\\wangz\\.conda\\envs\\NLP\\lib\\site-packages\\datasets\\packaged_modules\\csv\n",
            "03/15/2023 17:33:29 - INFO - datasets.builder - Generating dataset csv (C:/Users/wangz/.cache/huggingface/datasets/Ozziey___csv/Ozziey--poems_dataset-d150b0a822ff0b4c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
            "Downloading and preparing dataset csv/Ozziey--poems_dataset to C:/Users/wangz/.cache/huggingface/datasets/Ozziey___csv/Ozziey--poems_dataset-d150b0a822ff0b4c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n",
            "03/15/2023 17:33:29 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "03/15/2023 17:33:30 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/Ozziey/poems_dataset/resolve/9c11462190ebfeadc8617011e148312eef86f1f2/final_df_emotions%28remove-bias%29.csv not found in cache or force_download set to True, downloading to C:\\Users\\wangz\\.cache\\huggingface\\datasets\\downloads\\tmpfjysfw6n\n",
            "03/15/2023 17:33:30 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/Ozziey/poems_dataset/resolve/9c11462190ebfeadc8617011e148312eef86f1f2/final_df_emotions%28remove-bias%29.csv in cache at C:\\Users\\wangz\\.cache\\huggingface\\datasets\\downloads\\49b33ba4104560b52981734ba0e6a054ad01cac3180c07916b461c5732e5eb2f\n",
            "03/15/2023 17:33:30 - INFO - datasets.utils.file_utils - creating metadata file for C:\\Users\\wangz\\.cache\\huggingface\\datasets\\downloads\\49b33ba4104560b52981734ba0e6a054ad01cac3180c07916b461c5732e5eb2f\n",
            "03/15/2023 17:33:30 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "03/15/2023 17:33:30 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "03/15/2023 17:33:30 - INFO - datasets.builder - Generating train split\n",
            "03/15/2023 17:33:30 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset csv downloaded and prepared to C:/Users/wangz/.cache/huggingface/datasets/Ozziey___csv/Ozziey--poems_dataset-d150b0a822ff0b4c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n",
            "03/15/2023 17:33:32 - INFO - datasets.builder - Using custom data configuration Ozziey--poems_dataset-d150b0a822ff0b4c\n",
            "03/15/2023 17:33:32 - INFO - datasets.info - Loading Dataset Infos from C:\\Users\\wangz\\.conda\\envs\\NLP\\lib\\site-packages\\datasets\\packaged_modules\\csv\n",
            "03/15/2023 17:33:32 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "03/15/2023 17:33:32 - INFO - datasets.info - Loading Dataset info from C:\\Users\\wangz\\.cache\\huggingface\\datasets\\Ozziey___csv\\Ozziey--poems_dataset-d150b0a822ff0b4c\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n",
            "03/15/2023 17:33:32 - WARNING - datasets.builder - Found cached dataset csv (C:/Users/wangz/.cache/huggingface/datasets/Ozziey___csv/Ozziey--poems_dataset-d150b0a822ff0b4c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
            "03/15/2023 17:33:32 - INFO - datasets.info - Loading Dataset info from C:/Users/wangz/.cache/huggingface/datasets/Ozziey___csv/Ozziey--poems_dataset-d150b0a822ff0b4c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n",
            "03/15/2023 17:33:34 - INFO - datasets.builder - Using custom data configuration Ozziey--poems_dataset-d150b0a822ff0b4c\n",
            "03/15/2023 17:33:34 - INFO - datasets.info - Loading Dataset Infos from C:\\Users\\wangz\\.conda\\envs\\NLP\\lib\\site-packages\\datasets\\packaged_modules\\csv\n",
            "03/15/2023 17:33:34 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "03/15/2023 17:33:34 - INFO - datasets.info - Loading Dataset info from C:\\Users\\wangz\\.cache\\huggingface\\datasets\\Ozziey___csv\\Ozziey--poems_dataset-d150b0a822ff0b4c\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n",
            "03/15/2023 17:33:34 - WARNING - datasets.builder - Found cached dataset csv (C:/Users/wangz/.cache/huggingface/datasets/Ozziey___csv/Ozziey--poems_dataset-d150b0a822ff0b4c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
            "03/15/2023 17:33:34 - INFO - datasets.info - Loading Dataset info from C:/Users/wangz/.cache/huggingface/datasets/Ozziey___csv/Ozziey--poems_dataset-d150b0a822ff0b4c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n",
            "03/15/2023 17:34:11 - INFO - datasets.arrow_dataset - Caching processed dataset at C:\\Users\\wangz\\.cache\\huggingface\\datasets\\Ozziey___csv\\Ozziey--poems_dataset-d150b0a822ff0b4c\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-f00b659b167788cf.arrow\n",
            "03/15/2023 17:34:11 - INFO - datasets.arrow_dataset - Caching processed dataset at C:\\Users\\wangz\\.cache\\huggingface\\datasets\\Ozziey___csv\\Ozziey--poems_dataset-d150b0a822ff0b4c\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-1b72592bf90923c3.arrow\n",
            "03/15/2023 17:34:11 - INFO - datasets.arrow_dataset - Caching processed dataset at C:\\Users\\wangz\\.cache\\huggingface\\datasets\\Ozziey___csv\\Ozziey--poems_dataset-d150b0a822ff0b4c\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-d7e1093be740fb0f.arrow\n",
            "03/15/2023 17:34:11 - INFO - datasets.arrow_dataset - Caching processed dataset at C:\\Users\\wangz\\.cache\\huggingface\\datasets\\Ozziey___csv\\Ozziey--poems_dataset-d150b0a822ff0b4c\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-12c211d38311fe9b.arrow\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading readme:   0%|          | 0.00/171 [00:00<?, ?B/s]\n",
            "Downloading readme: 100%|██████████| 171/171 [00:00<00:00, 166kB/s]\n",
            "\n",
            "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "\n",
            "Downloading data:   0%|          | 0.00/400k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "Downloading data:  14%|█▍        | 57.3k/400k [00:00<00:00, 359kB/s]\u001b[A\n",
            "\n",
            "Downloading data:  73%|███████▎  | 291k/400k [00:00<00:00, 1.01MB/s]\u001b[A\n",
            "Downloading data: 100%|██████████| 400k/400k [00:00<00:00, 1.22MB/s]\n",
            "\n",
            "Downloading data files: 100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\n",
            "Downloading data files: 100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\n",
            "\n",
            "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1038.45it/s]\n",
            "\n",
            "Generating train split: 0 examples [00:00, ? examples/s]\n",
            "                                                        \n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 491.14it/s]\n",
            "\n",
            "Downloading (…)lve/main/config.json:   0%|          | 0.00/927 [00:00<?, ?B/s]\n",
            "Downloading (…)lve/main/config.json: 100%|██████████| 927/927 [00:00<00:00, 309kB/s]\n",
            "C:\\Users\\wangz\\.conda\\envs\\NLP\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\wangz\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "[INFO|configuration_utils.py:668] 2023-03-15 17:33:35,300 >> loading configuration file config.json from cache at C:\\Users\\wangz/.cache\\huggingface\\hub\\models--ismaelfaro--gpt2-poems.en\\snapshots\\0cf19ae5b1500bd3cd172571628596aa085655b3\\config.json\n",
            "[INFO|configuration_utils.py:720] 2023-03-15 17:33:35,301 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"ismaelfaro/gpt2-poems.en\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.27.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "\n",
            "Downloading (…)okenizer_config.json:   0%|          | 0.00/256 [00:00<?, ?B/s]\n",
            "Downloading (…)okenizer_config.json: 100%|██████████| 256/256 [00:00<00:00, 85.3kB/s]\n",
            "\n",
            "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]\n",
            "Downloading (…)olve/main/vocab.json: 100%|██████████| 798k/798k [00:01<00:00, 667kB/s]\n",
            "Downloading (…)olve/main/vocab.json: 100%|██████████| 798k/798k [00:01<00:00, 667kB/s]\n",
            "\n",
            "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\n",
            "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.37MB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.37MB/s]\n",
            "\n",
            "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]\n",
            "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 2.69MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 2.68MB/s]\n",
            "\n",
            "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]\n",
            "Downloading (…)cial_tokens_map.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 28.2kB/s]\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-03-15 17:33:41,684 >> loading file vocab.json from cache at C:\\Users\\wangz/.cache\\huggingface\\hub\\models--ismaelfaro--gpt2-poems.en\\snapshots\\0cf19ae5b1500bd3cd172571628596aa085655b3\\vocab.json\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-03-15 17:33:41,684 >> loading file merges.txt from cache at C:\\Users\\wangz/.cache\\huggingface\\hub\\models--ismaelfaro--gpt2-poems.en\\snapshots\\0cf19ae5b1500bd3cd172571628596aa085655b3\\merges.txt\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-03-15 17:33:41,684 >> loading file tokenizer.json from cache at C:\\Users\\wangz/.cache\\huggingface\\hub\\models--ismaelfaro--gpt2-poems.en\\snapshots\\0cf19ae5b1500bd3cd172571628596aa085655b3\\tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-03-15 17:33:41,684 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-03-15 17:33:41,684 >> loading file special_tokens_map.json from cache at C:\\Users\\wangz/.cache\\huggingface\\hub\\models--ismaelfaro--gpt2-poems.en\\snapshots\\0cf19ae5b1500bd3cd172571628596aa085655b3\\special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-03-15 17:33:41,684 >> loading file tokenizer_config.json from cache at C:\\Users\\wangz/.cache\\huggingface\\hub\\models--ismaelfaro--gpt2-poems.en\\snapshots\\0cf19ae5b1500bd3cd172571628596aa085655b3\\tokenizer_config.json\n",
            "\n",
            "Downloading pytorch_model.bin:   0%|          | 0.00/510M [00:00<?, ?B/s]\n",
            "Downloading pytorch_model.bin:   2%|▏         | 10.5M/510M [00:00<00:41, 11.9MB/s]\n",
            "Downloading pytorch_model.bin:   4%|▍         | 21.0M/510M [00:01<00:25, 19.5MB/s]\n",
            "Downloading pytorch_model.bin:   6%|▌         | 31.5M/510M [00:01<00:20, 23.6MB/s]\n",
            "Downloading pytorch_model.bin:   8%|▊         | 41.9M/510M [00:01<00:17, 27.1MB/s]\n",
            "Downloading pytorch_model.bin:  10%|█         | 52.4M/510M [00:02<00:18, 24.4MB/s]\n",
            "Downloading pytorch_model.bin:  12%|█▏        | 62.9M/510M [00:02<00:20, 21.3MB/s]\n",
            "Downloading pytorch_model.bin:  14%|█▍        | 73.4M/510M [00:03<00:20, 21.2MB/s]\n",
            "Downloading pytorch_model.bin:  16%|█▋        | 83.9M/510M [00:03<00:20, 20.6MB/s]\n",
            "Downloading pytorch_model.bin:  18%|█▊        | 94.4M/510M [00:04<00:20, 20.1MB/s]\n",
            "Downloading pytorch_model.bin:  21%|██        | 105M/510M [00:05<00:20, 19.7MB/s] \n",
            "Downloading pytorch_model.bin:  23%|██▎       | 115M/510M [00:05<00:20, 19.3MB/s]\n",
            "Downloading pytorch_model.bin:  25%|██▍       | 126M/510M [00:06<00:19, 19.5MB/s]\n",
            "Downloading pytorch_model.bin:  27%|██▋       | 136M/510M [00:06<00:20, 18.7MB/s]\n",
            "Downloading pytorch_model.bin:  29%|██▉       | 147M/510M [00:07<00:18, 19.6MB/s]\n",
            "Downloading pytorch_model.bin:  31%|███       | 157M/510M [00:07<00:18, 19.5MB/s]\n",
            "Downloading pytorch_model.bin:  33%|███▎      | 168M/510M [00:08<00:17, 19.5MB/s]\n",
            "Downloading pytorch_model.bin:  35%|███▍      | 178M/510M [00:08<00:17, 19.5MB/s]\n",
            "Downloading pytorch_model.bin:  37%|███▋      | 189M/510M [00:09<00:16, 19.5MB/s]\n",
            "Downloading pytorch_model.bin:  39%|███▉      | 199M/510M [00:09<00:16, 19.4MB/s]\n",
            "Downloading pytorch_model.bin:  41%|████      | 210M/510M [00:10<00:16, 18.5MB/s]\n",
            "Downloading pytorch_model.bin:  43%|████▎     | 220M/510M [00:11<00:15, 19.1MB/s]\n",
            "Downloading pytorch_model.bin:  45%|████▌     | 231M/510M [00:11<00:14, 19.3MB/s]\n",
            "Downloading pytorch_model.bin:  47%|████▋     | 241M/510M [00:12<00:13, 19.5MB/s]\n",
            "Downloading pytorch_model.bin:  49%|████▉     | 252M/510M [00:12<00:13, 19.4MB/s]\n",
            "Downloading pytorch_model.bin:  51%|█████▏    | 262M/510M [00:13<00:12, 19.4MB/s]\n",
            "Downloading pytorch_model.bin:  53%|█████▎    | 273M/510M [00:13<00:12, 19.2MB/s]\n",
            "Downloading pytorch_model.bin:  55%|█████▌    | 283M/510M [00:14<00:11, 19.3MB/s]\n",
            "Downloading pytorch_model.bin:  58%|█████▊    | 294M/510M [00:14<00:11, 19.3MB/s]\n",
            "Downloading pytorch_model.bin:  60%|█████▉    | 304M/510M [00:15<00:11, 18.7MB/s]\n",
            "Downloading pytorch_model.bin:  62%|██████▏   | 315M/510M [00:15<00:10, 19.3MB/s]\n",
            "Downloading pytorch_model.bin:  64%|██████▎   | 325M/510M [00:16<00:09, 19.5MB/s]\n",
            "Downloading pytorch_model.bin:  66%|██████▌   | 336M/510M [00:17<00:09, 18.6MB/s]\n",
            "Downloading pytorch_model.bin:  68%|██████▊   | 346M/510M [00:17<00:08, 19.5MB/s]\n",
            "Downloading pytorch_model.bin:  70%|██████▉   | 357M/510M [00:18<00:07, 19.4MB/s]\n",
            "Downloading pytorch_model.bin:  72%|███████▏  | 367M/510M [00:18<00:07, 19.3MB/s]\n",
            "Downloading pytorch_model.bin:  74%|███████▍  | 377M/510M [00:19<00:06, 19.1MB/s]\n",
            "Downloading pytorch_model.bin:  76%|███████▌  | 388M/510M [00:19<00:06, 18.3MB/s]\n",
            "Downloading pytorch_model.bin:  78%|███████▊  | 398M/510M [00:20<00:05, 19.5MB/s]\n",
            "Downloading pytorch_model.bin:  80%|████████  | 409M/510M [00:20<00:05, 19.3MB/s]\n",
            "Downloading pytorch_model.bin:  82%|████████▏ | 419M/510M [00:21<00:04, 18.8MB/s]\n",
            "Downloading pytorch_model.bin:  84%|████████▍ | 430M/510M [00:22<00:04, 19.0MB/s]\n",
            "Downloading pytorch_model.bin:  86%|████████▋ | 440M/510M [00:22<00:03, 18.3MB/s]\n",
            "Downloading pytorch_model.bin:  88%|████████▊ | 451M/510M [00:23<00:03, 19.0MB/s]\n",
            "Downloading pytorch_model.bin:  90%|█████████ | 461M/510M [00:23<00:02, 19.1MB/s]\n",
            "Downloading pytorch_model.bin:  92%|█████████▏| 472M/510M [00:24<00:02, 18.8MB/s]\n",
            "Downloading pytorch_model.bin:  95%|█████████▍| 482M/510M [00:24<00:01, 18.9MB/s]\n",
            "Downloading pytorch_model.bin:  97%|█████████▋| 493M/510M [00:25<00:00, 19.1MB/s]\n",
            "Downloading pytorch_model.bin:  99%|█████████▊| 503M/510M [00:25<00:00, 19.5MB/s]\n",
            "Downloading pytorch_model.bin: 100%|██████████| 510M/510M [00:26<00:00, 19.0MB/s]\n",
            "Downloading pytorch_model.bin: 100%|██████████| 510M/510M [00:26<00:00, 19.4MB/s]\n",
            "[INFO|modeling_utils.py:2403] 2023-03-15 17:34:08,878 >> loading weights file pytorch_model.bin from cache at C:\\Users\\wangz/.cache\\huggingface\\hub\\models--ismaelfaro--gpt2-poems.en\\snapshots\\0cf19ae5b1500bd3cd172571628596aa085655b3\\pytorch_model.bin\n",
            "[INFO|configuration_utils.py:575] 2023-03-15 17:34:09,206 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.27.0\"\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3032] 2023-03-15 17:34:10,564 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:3041] 2023-03-15 17:34:10,565 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ismaelfaro/gpt2-poems.en.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "[INFO|modeling_utils.py:2691] 2023-03-15 17:34:10,995 >> Generation config file not found, using a generation config created from the model config.\n",
            "\n",
            "Running tokenizer on dataset:   0%|          | 0/428 [00:00<?, ? examples/s]\n",
            "Running tokenizer on dataset: 100%|██████████| 428/428 [00:00<00:00, 594.51 examples/s]\n",
            "                                                                                       \n",
            "\n",
            "Running tokenizer on dataset:   0%|          | 0/22 [00:00<?, ? examples/s]\n",
            "                                                                           \n",
            "\n",
            "Grouping texts in chunks of 1024:   0%|          | 0/428 [00:00<?, ? examples/s]\n",
            "Grouping texts in chunks of 1024: 100%|██████████| 428/428 [00:00<00:00, 4196.33 examples/s]\n",
            "                                                                                            \n",
            "\n",
            "Grouping texts in chunks of 1024:   0%|          | 0/22 [00:00<?, ? examples/s]\n",
            "                                                                               \n",
            "\n",
            "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]\n",
            "Downloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 4.20MB/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"fine_tune.py\", line 625, in <module>\n",
            "    main()\n",
            "  File \"fine_tune.py\", line 541, in main\n",
            "    metric = evaluate.load(\"accuracy\")\n",
            "  File \"C:\\Users\\wangz\\.conda\\envs\\NLP\\lib\\site-packages\\evaluate\\loading.py\", line 732, in load\n",
            "    path, module_type=module_type, revision=revision, download_config=download_config, download_mode=download_mode\n",
            "  File \"C:\\Users\\wangz\\.conda\\envs\\NLP\\lib\\site-packages\\evaluate\\loading.py\", line 680, in evaluation_module_factory\n",
            "    raise e1 from None\n",
            "  File \"C:\\Users\\wangz\\.conda\\envs\\NLP\\lib\\site-packages\\evaluate\\loading.py\", line 638, in evaluation_module_factory\n",
            "    dynamic_modules_path=dynamic_modules_path,\n",
            "  File \"C:\\Users\\wangz\\.conda\\envs\\NLP\\lib\\site-packages\\evaluate\\loading.py\", line 493, in get_module\n",
            "    download_config=self.download_config,\n",
            "  File \"C:\\Users\\wangz\\.conda\\envs\\NLP\\lib\\site-packages\\evaluate\\loading.py\", line 266, in _download_additional_modules\n",
            "    f\"To be able to use {name}, you need to install the following dependencies\"\n",
            "ImportError: To be able to use evaluate-metric/accuracy, you need to install the following dependencies['sklearn'] using 'pip install sklearn' for instance'\n"
          ]
        }
      ],
      "source": [
        "!python fine_tune.py \\\n",
        "--per_device_train_batch_size 8 \\\n",
        "--per_device_eval_batch_size 8 \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--output_dir /tmp/poem_baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-2GnJODBdMaM"
      },
      "source": [
        "Vocabulary size : 50260\n",
        "\n",
        "Total number of tokens : 244000+\n",
        "\n",
        "Final loss : 3.346"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e5ODoTep7a3h"
      },
      "source": [
        "## Evaluate perplexity on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "sBkJ1KXh7eTi",
        "outputId": "960b4ff9-856b-4b88-8227-7705e3d7868e"
      },
      "outputs": [],
      "source": [
        "TEST_FILE='../../../dataset/valid.txt'\n",
        "CUDA_VISIBLE_DEVICES=\"1\" \n",
        "!python run_language_modeling.py \\\n",
        "--output_dir=$OUTPUT_DIR \\\n",
        "--model_type=gpt2 \\\n",
        "--model_name_or_path=$OUTPUT_DIR \\\n",
        "--do_eval \\\n",
        "--eval_data_file=$TEST_FILE \\\n",
        "--per_device_eval_batch_size=2 \\\n",
        "--line_by_line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X9Y1HEJ97oRR"
      },
      "source": [
        "## Text generation\n",
        "\n",
        "Setting prompt text: \"I always wonder\"\n",
        "\n",
        "Using top-k sampling decoder from GPT-2 (k=50 has been proven to be more effective at generating non-repetitive texts.) Other decoders are also available.\n",
        "\n",
        "Set <code>add_special_tokens=True</code> in the transformers/examples/text-generation/run_generation.py:\n",
        "<code>\n",
        "encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=True, return_tensors='pt')</code>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "n_WakQt07-sL",
        "outputId": "dcaace76-d490-4517-867b-3f42dd6b8004"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# os.chdir('./transformers/examples/text-generation')\n",
        "\n",
        "K=50\n",
        "CUDA_VISIBLE_DEVICES=\"1\" \n",
        "!python run_generation.py \\\n",
        "--model_type gpt2 \\\n",
        "--model_name_or_path $OUTPUT_DIR \\\n",
        "--length 300 \\\n",
        "--prompt \"<BOS>\" \\\n",
        "--stop_token \"<EOS>\" \\\n",
        "--k $K \\\n",
        "--num_return_sequences 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2aoxCk96i0b4"
      },
      "source": [
        "Wow. This is my first time using a pre-trained model on my data set for text generation (I tried to build my own LM using unidirectional LSTM and the results were underwhelming at best). \n",
        "These sentences really sound like me - I'm still processing it. Let me recover from this, and I would love to see what GPT-3 is capable of! \n",
        "\n",
        "(Although maybe we aren't ready for it? Maybe it's good it isn't open-source yet? Very curious)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "colab_type": "code",
        "id": "ojuTM1NcNoEK",
        "outputId": "424aca4e-7a93-40ce-dd01-eb99d4b79d6d"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "from google.colab import files\n",
        "print(os.getcwd())\n",
        "fles=glob.glob('../../../model/checkpoint-1000/*')\n",
        "print(fles)\n",
        "for file in fles:\n",
        "  files.download(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "cy6n3l_XP8Vi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOL4rf+yBkuBNzWQG3oZD5Q",
      "collapsed_sections": [],
      "include_colab_link": true,
      "mount_file_id": "1SsYKniB48_XJTuSX26nRFWYa9Y51BMay",
      "name": "text-generation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
